{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import urllib.request, json, re\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "import random\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassformat för lagring och serialisering\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DocList:\n",
    "    data = []\n",
    "    def __init__(self):\n",
    "        self.name='MotionList'\n",
    "        self.data = []\n",
    "    def reprJSON(self):\n",
    "        return dict(name=self.name, data=self.data) \n",
    "    \n",
    "class Motion:\n",
    "    def __init__(self):\n",
    "        self.text=''\n",
    "        self.title=''\n",
    "        self.subtype=''\n",
    "        self.meta = {}\n",
    "    def reprJSON(self):\n",
    "        return dict(text=self.text, subtype=self.subtype, title=self.title, meta=self.meta) \n",
    "    \n",
    "class DocSerializer(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if hasattr(obj,'reprJSON'):\n",
    "            return obj.reprJSON()\n",
    "        else:\n",
    "            return json.JSONEncoder.default(self, obj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import och skapande av datafil\n",
    "denna koden ska bara köras en gång vid inläsning av data från riksdagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totalt antal dokument: 2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def import_web_data(datasize=3):\n",
    "    pages = int(np.floor(datasize/201))+1\n",
    "    #http://data.riksdagen.se/dokumentlista/?doktyp=mot\n",
    "    #http://data.riksdagen.se/dokument/H502225.text (exempeldok.)\n",
    "    #\"http://data.riksdagen.se/anforandelista/?rm=&anftyp=Nej&d=&ts=&parti=s&iid=0218878014918&sz=\"+str(datasize)+\"&utformat=json\"\n",
    "    #http://data.riksdagen.se/anforandelista/?rm=&anftyp=&d=&ts=&parti=&iid=&sz=10&utformat=xml\n",
    "    motioner = DocList()\n",
    "    for page in range(pages):\n",
    "        with urllib.request.urlopen(\"http://data.riksdagen.se/dokumentlista/?doktyp=mot&sz=\"+str(datasize)+\"&utformat=json&p=\"+str(page+1)) as url:\n",
    "            rawdata = json.loads(url.read().decode())\n",
    "            motionslista = rawdata['dokumentlista']['dokument']\n",
    "            for meta in motionslista:\n",
    "                if(not not meta):\n",
    "                    motion = Motion()\n",
    "                    with urllib.request.urlopen('http:' + meta['dokument_url_text']) as text:\n",
    "                        motion.text = re.sub('\\n','',text.read().decode())\n",
    "                        #motion.meta = meta (save space appr 30%)\n",
    "                        motion.title = meta['titel']\n",
    "                        motion.subtype = meta['subtyp']\n",
    "\n",
    "                    motioner.data.append(motion)\n",
    "\n",
    "     \n",
    "    toJson = json.dumps(motioner.reprJSON(), cls=DocSerializer,  sort_keys=True, indent=4)\n",
    "    with open('motioner_big.txt','w',encoding='utf-8') as f:\n",
    "        \n",
    "        f.write(toJson) #json.dumps(motions.reprJSON()))#json.dumps(motions, sort_keys=True, indent=4))\n",
    "        \n",
    "    return toJson, motioner\n",
    "\n",
    "#datasize är antal dokument vi behöver förmodligen ett hundratal iaf. (nedan är bortkommenterad om man har fil)\n",
    "data_ex, motionList = import_web_data(datasize=2000)\n",
    "print(\"totalt antal dokument: \" + str(len(motionList.data)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_to_json(filename):\n",
    "    with open(filename,encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "        return data\n",
    "\n",
    "def json_to_vec(jsondata):\n",
    "    docs = []\n",
    "    words = []\n",
    "    for sample in jsondata:\n",
    "        #remove non letters\n",
    "        cleaned = sample[\"text\"].lower()\n",
    "        cleaned = re.sub('[\\r\\r]', '', cleaned) #måste vara såhär (då ord kan vara delade varsomhelst med \\r\\r)\n",
    "        cleaned = re.sub('[\\n|\\)|\\(]', ' ', cleaned)\n",
    "        PERMITTED_CHARS = \"abcdefghijklmnopqrstuvwxyzåäöABCDEFGHIJKLMNOPQRSTUVWXYZÅÄÖ \"\n",
    "\n",
    "        cleaned = \"\".join(c for c in cleaned if c in PERMITTED_CHARS)\n",
    "        #cleaned = re.sub(r'^\\w', '', sample[\"text\"].lower())\n",
    "        #cleaned =  sample[\"text\"].lower()\n",
    "        #remove space\n",
    "        cleaned =' '.join( [x for x in re.split(r'[\\u200b\\s]+', cleaned, flags=re.UNICODE) if x != ''])\n",
    "        words += cleaned.lower().split()\n",
    "        docs.append(cleaned)\n",
    "\n",
    "    #print(speech)\n",
    "    #data = tf.compat.as_str(speech).split()\n",
    "    orig_dic = dict(list(zip(words,words)))\n",
    "    return docs, orig_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hemavdrag', 'hembesök', 'hembygd', 'hembygden', 'hemelektronik', 'hemifrån', 'hemkunskap', 'hemkunskapsämnet', 'hemland', 'hemlandet']\n"
     ]
    }
   ],
   "source": [
    "jsondata = file_to_json(\"motioner.txt\")\n",
    "\n",
    "docs, orig_dic = json_to_vec(jsondata['data'])\n",
    "\n",
    "lista = list(set(orig_dic.keys()))\n",
    "lista.sort()\n",
    "print(lista[6300:6310])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  läs från fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion till riksdagenav oscar sjöstedt och dennis dioukarev båda sd med anledning av prop bättre förutsättningar för fondsparande och hållbara valförslag till riksdagsbeslutriksdagen avslår förslaget i de delar som avser krav på att tillhandahållande av hållbarhetsinformation regleras i lag och därtill knutna bestämmelser i förslagen till lag om värdepappersfonder och lag om förvaltare av alternativa investeringsfonder avsnitt och motiveringregeringens föreslår att genom lag reglera att fonder ska lämna information om förvaltning med avseende på olika hållbarhetsaspekter så som miljö sociala förhållanden personal respekt för mänskliga rättigheter och korruption vidare ska samtliga fonder vare sig de beaktar hållbarhetsaspekter eller ej också åläggas med informationskrav om att rapportera detta det finns idag flera internationellt erkända alternativ för hållbarhetsinformation fn initiativet principles for responsible investments pri som förtydligar redovisning inom hållbarhet med kapitalförvaltningsfrågor utöver detta finns också nordiska initiativ som svanmärkningen vilket ger en möjlighet för fonder med en tydlig hållbarhetsprofil att få sitt hållbarhetsarbete granskat och i slutändan certifierat även den oberoende organisationen swedish sustainable investment forum swesif tillhandahåller information för ickeprofessionella sparare i form av en hållbarhetsprofil morningstar är ett annat exempel som gör en helhetsbedömning av olika fonders hållbarhetsarbete vidare finns nationella branschinitiativ från fondbolagens förening med förslag på riktlinjer kring korrekta uttryck och ickemissvisande formuleringar vid marknadsföring samt krav på en hållbarhetsöversikt där hållbarhetsarbetet beskrivs med konkreta exempel idag ges även investerare som söker ett hållbart fondalternativ sett till esg standarder fullgoda möjligheter att investera i den breda och växande mängden hållbarhetsfonder dessa fonders placeringsstrategi är att ta hänsyn till företagens csrprogram samtidigt som de certifierar sig själv genom exempelvis sk svanmärkning eller användandet av priredovisning för den genomsnittliga fondspararen finns således redan idag lättillgänglig information och goda möjligheter till att investera i fonder på grundval av deras hållbarhetden svenska lagstiftningen skulle med detta krav avvika från eustandarden samtidigt som dessa bolag erläggs med en ökad administrativ börda det kan därför ifrågasättas hur effektiv en reglering kring hållbarhetsinformationen kan anses vara när den inte är tillämplig på värdpappersbaserade fonder vars hemvist är utanför sverige det skulle leda till en informationsasymmetri där utländska värdepappersfonder får en konkurrensfördel i och med avsaknaden av administrationskrav för att rapportera hållbarhetsinformation slutligen anför finansinspektionen i sitt remissvar att det är oklart vad som avses med begreppet hållbarhet avsaknaden av kontrollmekanismer av fondernas hållbarhetsinformation kan i värsta fall leda till sk greenwashing med detta menas att fondförvaltarna felaktigt marknadsför fonden som mer hållbar än vad den egentligen är i syfte av att göra den mer attraktiv hos sparare sverigedemokraternas uppfattning är att utvecklingen av hållbarhetsrapportering bäst sker genom självreglering vilket redan bevisligen finns inom branschen det påpekas även av flera remissinstanser att det finns en överhängande risk att en lagreglering snabbt kan bli omodern och i sämsta fall kontraproduktiv på dessa grunder yrkar sverigedemokraterna avslag på förslaget i de delar som avser krav gällande tillhandahållande av hållbarhetsinformationoscar sjöstedt sd dennis dioukarev sd\n"
     ]
    }
   ],
   "source": [
    "jsondata = file_to_json(\"motioner.txt\")\n",
    "\n",
    "docs, orig_dic = json_to_vec(jsondata['data'])\n",
    "\n",
    "print(docs[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Lemmatisering av alla ord\n",
    "Här måste vi gå igenom alla ord och rensa upp för att sedan ersätta ord i texten med en mindre ordrymd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 19479 to: 11476\n"
     ]
    }
   ],
   "source": [
    "def indexOf(text, elem):\n",
    "        try:\n",
    "            thing_index = text.index(elem)\n",
    "            return thing_index\n",
    "        except ValueError:\n",
    "            return -1\n",
    "\n",
    "        \n",
    "#skapar en dictionary över gammalt till nytt ord så man kan köra find-replace med den senare.\n",
    "def lemmatize_and_clean(orig_dic, sensitivity=.5):        \n",
    "    # a jph production yeahh\n",
    "    \n",
    "    # unika ord (set)\n",
    "    myset = set(list(orig_dic.keys()))\n",
    "    # till lista igen\n",
    "    all_words = list(myset)\n",
    "    #sortera bokstav\n",
    "    all_words.sort(reverse=False)\n",
    "    \n",
    " \n",
    "    orig = all_words\n",
    "    newlist = []\n",
    "    #regexp = re.compile('(.?[0-9]+)|^[/&%\\[=\\–]') #rensa tecken\n",
    "    for i in range(len(orig)):\n",
    "        #if(regexp.match(orig[i])):\n",
    "        #    newlist.append('')\n",
    "        #else:\n",
    "            fidx = 0\n",
    "            if(len(newlist)>0):\n",
    "                suffix=0\n",
    "                if(len(orig[i])>7):\n",
    "                    suffix = 2\n",
    "                fidx = (indexOf(orig[i],newlist[i-1][:len(orig[i])-suffix])+1)\n",
    "            #kolla om längd på aktuellt ersättningsord (newlist[i-1]) är över dellängd av nuvarande ord (eg .6 = 60%)\n",
    "            #ifall denna skillnad blir för stor byt ersättningsord\n",
    "            if(fidx != 0 and ((1.0*len(newlist[i-1])/len(orig[i]) >=sensitivity) and len(newlist[i-1]) > 4)):\n",
    "                newlist.append(newlist[i-1])\n",
    "            else:\n",
    "                newlist.append(orig[i])\n",
    "    \n",
    "    \n",
    "    return dict(list(zip(orig,newlist)))\n",
    "\n",
    "\n",
    "orig_dic  = lemmatize_and_clean(orig_dic,sensitivity=.4)\n",
    "\n",
    "print(\"from \"+str(len(orig_dic))+\" to: \"+str(len(set(list(orig_dic.values())))))\n",
    "\n",
    "#print(orig_dic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: använd wdict på alla textdokument för att ersätta affärens, affärsmodell, affärsrunda... med affär (enl ovan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "ta bort ord som inte säger något som prepositioner mm. Min naiva approach är att kolla upp de vanligaste orden och ta bort dem. Tror dock man ska kolla (http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/)\n",
    "St (pip install stop-words yay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "def remove_stop_words(dic):\n",
    "    stop_words = get_stop_words('sv')\n",
    "    return {k:('' if (dic[k] in stop_words) else dic[k]) for k in dic}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove Stopwords from 90117 to: 90005\n",
      "remove suffix from 90005 to: 50527\n"
     ]
    }
   ],
   "source": [
    "jsondata = file_to_json(\"motioner_big.txt\")\n",
    "texts, orig_dic = json_to_vec(jsondata['data'])\n",
    "\n",
    "stop_dic = remove_stop_words(orig_dic)\n",
    "print(\"remove Stopwords from \"+str(len(orig_dic))+\" to: \"+str(len(set(list(stop_dic.values())))))\n",
    "\n",
    "lemma_dic = lemmatize_and_clean(stop_dic,sensitivity=.4)\n",
    "print(\"remove suffix from \"+str(len(set(list(stop_dic.values()))))+\" to: \"+str(len(set(list(lemma_dic.values())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(orig_dic):\n",
    "    before = str(len(orig_dic.keys()))\n",
    "    dic = remove_stop_words(orig_dic)\n",
    "    dic = lemmatize_and_clean(dic,sensitivity=.4)\n",
    "    print(\"Limit words from \"+before+\" to: \"+str(len(set(list(dic.values()))))+\"\\n\")\n",
    "    return dic\n",
    "\n",
    "\n",
    "##SLOW AS FUCK.\n",
    "def preprocess(texts,dic):\n",
    "    lst = [re.escape(key) for key in dic.keys()]\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(lst) + r')\\b')\n",
    "    cleaned = []\n",
    "    for text in texts:\n",
    "        cleaned.append(pattern.sub(lambda x: dic[x.group()], text))\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_clean(file_in, file_out):\n",
    "    jsondata = file_to_json(file_in)\n",
    "    texts, orig_dic = json_to_vec(jsondata['data'])\n",
    "    dic = build_dict(orig_dic)\n",
    "    texts = preprocess(texts,dic)\n",
    "    with open(file_out,'w',encoding='utf-8') as f:\n",
    "        f.write(json.dumps(texts, sort_keys=True, indent=4))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_clean('motioner_big.txt','motioner_cleaned.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ta ut alla dokument\n",
    "jsondata = file_to_json(\"motioner_big.txt\")\n",
    "texts, orig_dic = json_to_vec(jsondata['data'])\n",
    "\n",
    "dic = build_dict(orig_dic)\n",
    "#print(dic)\n",
    "ts = texts[2:3]\n",
    "print(ts,len(ts[0]))\n",
    "texts = preprocess(ts,dic)\n",
    "\n",
    "print(\"\\n\",texts,len(texts[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nästa steg - Bag of words\n",
    "skapa vektor av alla ord. Eg bara köra list(set(orig_dic.values())) \n",
    "Skapa np.arrays där varje dokument har en siffra i varje position i ovan vektor.\n",
    "Detta blir då ett training example eller ett x i X.\n",
    "\n",
    "# NMF\n",
    "\n",
    "för att sedan få matrisrepresentation som går att köra NonNegative Matrix Factorization på.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_to_vec(sample, wi):\n",
    "    vocabular = wi\n",
    "    sample_words = np.array(sample.split(\" \"))\n",
    "    vec = np.array([wi[x] for x in sample_words])\n",
    "    one_hot = np.zeros([len(vec), len(vocabular)])\n",
    "    one_hot[np.arange(len(vec)),vec] = 1\n",
    "    one_hot = np.sum(one_hot,axis=0)\n",
    "    return vec,one_hot\n",
    "\n",
    "#generera index - ord till id och id till ord\n",
    "def gen_idx(texts):\n",
    "    all_unique_words = list(set((''.join(texts)).split(\" \")))\n",
    "    word_to_idx = {i:w for w,i in enumerate(all_unique_words)}\n",
    "    idx_to_word = {i:w for i,w in enumerate(all_unique_words)}   \n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "\n",
    "# samples_to_train:\n",
    "# [sampletxt, sampletxt ...] => [n x m], [word->index], [index->word]\n",
    "# där n = ordlistas längd (features) och m = antal dokument\n",
    "def samples_to_train(samples):\n",
    "    vocabulary, iw = gen_idx(samples)\n",
    "    sample_vector = []\n",
    "    for sample in samples:\n",
    "        _, x = sample_to_vec(sample, vocabulary)\n",
    "        sample_vector.append(x)\n",
    "    return np.matrix(sample_vector).reshape((len(samples),len(vocabulary))).T, vocabulary, iw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# går inte att köra för preprocess är slött\n",
    "Detta steg är viktigt för att få bort massa skräp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit words from 90117 to: 50527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_clean('motioner_big.txt','motioner_cleaned.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsondata = file_to_json(\"motioner.txt\")\n",
    "texts, orig_dic = json_to_vec(jsondata['data'])\n",
    "\n",
    "X_samp, wi, iw = samples_to_train(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.66585801  0.08635763]\n",
      " [ 0.73400202  2.06222242]\n",
      " [ 1.86096512  0.31755657]]\n",
      "[[ 0.38534454  0.4675586   0.00799441]\n",
      " [ 0.          0.34002261  0.94060871]\n",
      " [ 1.34627306  0.49279364  0.21257051]\n",
      " [ 1.49619407  0.31673295  0.68516976]\n",
      " [ 0.26846283  0.          2.44645467]\n",
      " [ 2.44632733  0.2376055   0.94057293]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=3, init='random', random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_\n",
    "print(H)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 4\n",
    "model = NMF(n_components=topics, init='random', random_state=0)\n",
    "W = model.fit_transform(X_samp.T)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 4)\n",
      "(4, 19487)\n"
     ]
    }
   ],
   "source": [
    "print(W.shape) #dokument och topic\n",
    "print(H.shape) #topics och ord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att\n",
      "i\n",
      "som\n",
      "och\n",
      "för\n",
      "det\n",
      "av\n",
      "en\n",
      "om\n",
      "till\n",
      "är\n",
      "ett\n",
      "detta\n",
      "den\n",
      "sig\n",
      "på\n",
      "med\n",
      "ska\n",
      "inte\n",
      "har\n"
     ]
    }
   ],
   "source": [
    "#topic 1 - detta verkar inte funka riktigt då jag inte tar bort ord som förekommer i alla dok.\n",
    "topic = 1\n",
    "t1 = dict(zip(H[topic],range(len(H[topic]))))\n",
    "t1 = sorted(t1.items(),reverse=True)\n",
    "for w,idx in t1[:20]:\n",
    "    print(iw[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: ska bör sverige regeringenriksdagen bakom ställer motionen anförs tillkännager regeringen\n",
      "Topic #1: miljoner kronor beräknas minska år föreslås följd anslaget belopp centerpartiets\n",
      "Topic #2: riksdagenmotionen utgår historiska hindras hindrar hindra hinder hen hemvist hemmet\n",
      "Topic #3: skalin johnny sd tillkännajohnny bör anförs ges motionen sista regeringen\n",
      "\n",
      "samples, max words/doc:(200, 3000)\n"
     ]
    }
   ],
   "source": [
    "#SciPy färdiga NMF med tfidf (samma som ovan, fast man slipper slå upp allt mot arrayer samt rensa ord.)\n",
    "topics = 4\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=3000,\n",
    "                                   stop_words=get_stop_words('sv'))\n",
    "X_samp = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "\n",
    "nmf = NMF(n_components=4, random_state=1,shuffle=True,\n",
    "          alpha=.1, l1_ratio=.7).fit(X_samp)\n",
    "\n",
    "\n",
    "vocabulary = tfidf_vectorizer.get_feature_names()\n",
    "#print(vocabulary)\n",
    "print_top_words(nmf, vocabulary, 10)\n",
    "print(\"samples, max words/doc:\" + str(X_samp.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
