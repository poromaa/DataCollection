{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import urllib.request, json, re\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "import random\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassformat för lagring och serialisering\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DocList:\n",
    "    data = []\n",
    "    def __init__(self):\n",
    "        self.name='MotionList'\n",
    "        self.data = []\n",
    "    def reprJSON(self):\n",
    "        return dict(name=self.name, data=self.data) \n",
    "    \n",
    "class Motion:\n",
    "    def __init__(self):\n",
    "        self.text=''\n",
    "        self.title=''\n",
    "        self.subtype=''\n",
    "        self.meta = {}\n",
    "    def reprJSON(self):\n",
    "        return dict(text=self.text, subtype=self.subtype, title=self.title, meta=self.meta) \n",
    "    \n",
    "class DocSerializer(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if hasattr(obj,'reprJSON'):\n",
    "            return obj.reprJSON()\n",
    "        else:\n",
    "            return json.JSONEncoder.default(self, obj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import och skapande av datafil\n",
    "denna koden ska bara köras en gång vid inläsning av data från riksdagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totalt antal dokument: 200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def import_web_data(datasize=3):\n",
    "\n",
    "    #http://data.riksdagen.se/dokumentlista/?doktyp=mot\n",
    "    #http://data.riksdagen.se/dokument/H502225.text (exempeldok.)\n",
    "    #\"http://data.riksdagen.se/anforandelista/?rm=&anftyp=Nej&d=&ts=&parti=s&iid=0218878014918&sz=\"+str(datasize)+\"&utformat=json\"\n",
    "    #http://data.riksdagen.se/anforandelista/?rm=&anftyp=&d=&ts=&parti=&iid=&sz=10&utformat=xml\n",
    "    motioner = DocList()\n",
    "    with urllib.request.urlopen(\"http://data.riksdagen.se/dokumentlista/?doktyp=mot&sz=\"+str(datasize)+\"&utformat=json\") as url:\n",
    "        rawdata = json.loads(url.read().decode())\n",
    "        motionslista = rawdata['dokumentlista']['dokument']\n",
    "        for meta in motionslista:\n",
    "            if(not not meta):\n",
    "                motion = Motion()\n",
    "                with urllib.request.urlopen('http:' + meta['dokument_url_text']) as text:\n",
    "                    motion.text = re.sub('\\n','',text.read().decode())\n",
    "                    #motion.meta = meta (save space appr 30%)\n",
    "                    motion.title = meta['titel']\n",
    "                    motion.subtype = meta['subtyp']\n",
    "\n",
    "                motioner.data.append(motion)\n",
    "\n",
    "     \n",
    "    toJson = json.dumps(motioner.reprJSON(), cls=DocSerializer,  sort_keys=True, indent=4)\n",
    "    with open('motioner.txt','w',encoding='utf-8') as f:\n",
    "        \n",
    "        f.write(toJson) #json.dumps(motions.reprJSON()))#json.dumps(motions, sort_keys=True, indent=4))\n",
    "        \n",
    "    return toJson, motioner\n",
    "\n",
    "#datasize är antal dokument vi behöver förmodligen ett hundratal iaf. (nedan är bortkommenterad om man har fil)\n",
    "#data_ex, motionList = import_web_data(datasize=500)\n",
    "print(\"totalt antal dokument: \" + str(len(motionList.data)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_json(filename):\n",
    "    with open(filename,encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "        return data\n",
    "    \n",
    "def json_to_vec(jsondata):\n",
    "    docs = []\n",
    "    words = []\n",
    "    for sample in jsondata:\n",
    "        #remove non letters\n",
    "        cleaned = sample[\"text\"].lower()\n",
    "        cleaned = re.sub('[\\r\\r]', '', cleaned) #måste vara såhär (då ord kan vara delade varsomhelst med \\r\\r)\n",
    "        cleaned = re.sub('[\\n|\\)|\\(]', ' ', cleaned)\n",
    "        PERMITTED_CHARS = \"abcdefghijklmnopqrstuvwxyzåäöABCDEFGHIJKLMNOPQRSTUVWXYZÅÄÖ \"\n",
    "\n",
    "        cleaned = \"\".join(c for c in cleaned if c in PERMITTED_CHARS)\n",
    "        #cleaned = re.sub(r'^\\w', '', sample[\"text\"].lower())\n",
    "        #cleaned =  sample[\"text\"].lower()\n",
    "        #remove space\n",
    "        cleaned =' '.join( [x for x in re.split(r'[\\u200b\\s]+', cleaned, flags=re.UNICODE) if x != ''])\n",
    "        words += cleaned.lower().split()\n",
    "        docs.append(cleaned)\n",
    "\n",
    "    #print(speech)\n",
    "    #data = tf.compat.as_str(speech).split()\n",
    "    orig_dic = dict(list(zip(words,words)))\n",
    "    return docs, orig_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hemavdrag', 'hembesök', 'hembygd', 'hembygden', 'hemelektronik', 'hemifrån', 'hemkunskap', 'hemkunskapsämnet', 'hemland', 'hemlandet']\n"
     ]
    }
   ],
   "source": [
    "jsondata = file_to_json(\"motioner.txt\")\n",
    "\n",
    "docs, orig_dic = json_to_vec(jsondata['data'])\n",
    "\n",
    "lista = list(set(orig_dic.keys()))\n",
    "lista.sort()\n",
    "print(lista[6300:6310])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  läs från fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion till riksdagenav oscar sjöstedt och dennis dioukarev båda sd med anledning av prop bättre förutsättningar för fondsparande och hållbara valförslag till riksdagsbeslutriksdagen avslår förslaget i de delar som avser krav på att tillhandahållande av hållbarhetsinformation regleras i lag och därtill knutna bestämmelser i förslagen till lag om värdepappersfonder och lag om förvaltare av alternativa investeringsfonder avsnitt och motiveringregeringens föreslår att genom lag reglera att fonder ska lämna information om förvaltning med avseende på olika hållbarhetsaspekter så som miljö sociala förhållanden personal respekt för mänskliga rättigheter och korruption vidare ska samtliga fonder vare sig de beaktar hållbarhetsaspekter eller ej också åläggas med informationskrav om att rapportera detta det finns idag flera internationellt erkända alternativ för hållbarhetsinformation fn initiativet principles for responsible investments pri som förtydligar redovisning inom hållbarhet med kapitalförvaltningsfrågor utöver detta finns också nordiska initiativ som svanmärkningen vilket ger en möjlighet för fonder med en tydlig hållbarhetsprofil att få sitt hållbarhetsarbete granskat och i slutändan certifierat även den oberoende organisationen swedish sustainable investment forum swesif tillhandahåller information för ickeprofessionella sparare i form av en hållbarhetsprofil morningstar är ett annat exempel som gör en helhetsbedömning av olika fonders hållbarhetsarbete vidare finns nationella branschinitiativ från fondbolagens förening med förslag på riktlinjer kring korrekta uttryck och ickemissvisande formuleringar vid marknadsföring samt krav på en hållbarhetsöversikt där hållbarhetsarbetet beskrivs med konkreta exempel idag ges även investerare som söker ett hållbart fondalternativ sett till esg standarder fullgoda möjligheter att investera i den breda och växande mängden hållbarhetsfonder dessa fonders placeringsstrategi är att ta hänsyn till företagens csrprogram samtidigt som de certifierar sig själv genom exempelvis sk svanmärkning eller användandet av priredovisning för den genomsnittliga fondspararen finns således redan idag lättillgänglig information och goda möjligheter till att investera i fonder på grundval av deras hållbarhetden svenska lagstiftningen skulle med detta krav avvika från eustandarden samtidigt som dessa bolag erläggs med en ökad administrativ börda det kan därför ifrågasättas hur effektiv en reglering kring hållbarhetsinformationen kan anses vara när den inte är tillämplig på värdpappersbaserade fonder vars hemvist är utanför sverige det skulle leda till en informationsasymmetri där utländska värdepappersfonder får en konkurrensfördel i och med avsaknaden av administrationskrav för att rapportera hållbarhetsinformation slutligen anför finansinspektionen i sitt remissvar att det är oklart vad som avses med begreppet hållbarhet avsaknaden av kontrollmekanismer av fondernas hållbarhetsinformation kan i värsta fall leda till sk greenwashing med detta menas att fondförvaltarna felaktigt marknadsför fonden som mer hållbar än vad den egentligen är i syfte av att göra den mer attraktiv hos sparare sverigedemokraternas uppfattning är att utvecklingen av hållbarhetsrapportering bäst sker genom självreglering vilket redan bevisligen finns inom branschen det påpekas även av flera remissinstanser att det finns en överhängande risk att en lagreglering snabbt kan bli omodern och i sämsta fall kontraproduktiv på dessa grunder yrkar sverigedemokraterna avslag på förslaget i de delar som avser krav gällande tillhandahållande av hållbarhetsinformationoscar sjöstedt sd dennis dioukarev sd\n"
     ]
    }
   ],
   "source": [
    "jsondata = file_to_json(\"motioner.txt\")\n",
    "\n",
    "docs, orig_dic = json_to_vec(jsondata['data'])\n",
    "\n",
    "print(docs[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Lemmatisering av alla ord\n",
    "Här måste vi gå igenom alla ord och rensa upp för att sedan ersätta ord i texten med en mindre ordrymd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 19479 to: 11476\n"
     ]
    }
   ],
   "source": [
    "def indexOf(text, elem):\n",
    "        try:\n",
    "            thing_index = text.index(elem)\n",
    "            return thing_index\n",
    "        except ValueError:\n",
    "            return -1\n",
    "\n",
    "        \n",
    "#skapar en dictionary över gammalt till nytt ord så man kan köra find-replace med den senare.\n",
    "def lemmatize_and_clean(orig_dic, sensitivity=.5):        \n",
    "    # a jph production yeahh\n",
    "    \n",
    "    # unika ord (set)\n",
    "    myset = set(list(orig_dic.keys()))\n",
    "    # till lista igen\n",
    "    all_words = list(myset)\n",
    "    #sortera bokstav\n",
    "    all_words.sort(reverse=False)\n",
    "    \n",
    " \n",
    "    orig = all_words\n",
    "    newlist = []\n",
    "    #regexp = re.compile('(.?[0-9]+)|^[/&%\\[=\\–]') #rensa tecken\n",
    "    for i in range(len(orig)):\n",
    "        #if(regexp.match(orig[i])):\n",
    "        #    newlist.append('')\n",
    "        #else:\n",
    "            fidx = 0\n",
    "            if(len(newlist)>0):\n",
    "                suffix=0\n",
    "                if(len(orig[i])>7):\n",
    "                    suffix = 2\n",
    "                fidx = (indexOf(orig[i],newlist[i-1][:len(orig[i])-suffix])+1)\n",
    "            #kolla om längd på aktuellt ersättningsord (newlist[i-1]) är över dellängd av nuvarande ord (eg .6 = 60%)\n",
    "            #ifall denna skillnad blir för stor byt ersättningsord\n",
    "            if(fidx != 0 and ((1.0*len(newlist[i-1])/len(orig[i]) >=sensitivity) and len(newlist[i-1]) > 4)):\n",
    "                newlist.append(newlist[i-1])\n",
    "            else:\n",
    "                newlist.append(orig[i])\n",
    "    \n",
    "    \n",
    "    return dict(list(zip(orig,newlist)))\n",
    "\n",
    "\n",
    "orig_dic  = lemmatize_and_clean(orig_dic,sensitivity=.4)\n",
    "\n",
    "print(\"from \"+str(len(orig_dic))+\" to: \"+str(len(set(list(orig_dic.values())))))\n",
    "\n",
    "#print(orig_dic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: använd wdict på alla textdokument för att ersätta affärens, affärsmodell, affärsrunda... med affär (enl ovan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "ta bort ord som inte säger något som prepositioner mm. Min naiva approach är att kolla upp de vanligaste orden och ta bort dem. Tror dock man ska kolla (http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/)\n",
    "St (pip install stop-words yay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "def remove_stop_words(dic):\n",
    "    stop_words = get_stop_words('sv')\n",
    "    return {k:('' if (dic[k] in stop_words) else dic[k]) for k in dic}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove Stopwords from 19479 to: 19373\n",
      "remove suffix from 19373 to: 11476\n"
     ]
    }
   ],
   "source": [
    "jsondata = file_to_json(\"motioner.txt\")\n",
    "texts, orig_dic = json_to_vec(jsondata['data'])\n",
    "\n",
    "stop_dic = remove_stop_words(orig_dic)\n",
    "print(\"remove Stopwords from \"+str(len(orig_dic))+\" to: \"+str(len(set(list(stop_dic.values())))))\n",
    "\n",
    "lemma_dic = lemmatize_and_clean(stop_dic,sensitivity=.4)\n",
    "print(\"remove suffix from \"+str(len(set(list(stop_dic.values()))))+\" to: \"+str(len(set(list(lemma_dic.values())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(orig_dic):\n",
    "    before = str(len(orig_dic.keys()))\n",
    "    dic = remove_stop_words(orig_dic)\n",
    "    dic = lemmatize_and_clean(dic,sensitivity=.4)\n",
    "    print(\"Limit words from \"+before+\" to: \"+str(len(set(list(dic.values()))))+\"\\n\")\n",
    "    return dic\n",
    "\n",
    "def preprocess(texts,dic):\n",
    "    lst = [re.escape(key) for key in dic.keys()]\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(lst) + r')\\b')\n",
    "    cleaned = []\n",
    "    for text in texts:\n",
    "        cleaned.append(pattern.sub(lambda x: dic[x.group()], text))\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_clean(file_in, file_out):\n",
    "    jsondata = file_to_json(file_in)\n",
    "    texts, orig_dic = json_to_vec(jsondata['data'])\n",
    "    dic = build_dict(orig_dic)\n",
    "    texts = preprocess(texts,dic)\n",
    "    with open(file_out,'w',encoding='utf-8') as f:\n",
    "        f.write(json.dumps(texts, sort_keys=True, indent=4))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit words from 19479 to: 11476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_clean('motioner.txt','motioner_cleaned.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit words from 19479 to: 11476\n",
      "\n",
      "['motion till riksdagenav olle felten och david lång båda sd med anledning av prop mervärdesskatt vid omsättning som avser fartyg samt deklarationstidpunkt i vissa fallförslag till riksdagsbeslutriksdagen ställer sig bakom det som anförs i motionen om att även sjöfart som går mellan två stater där territorialvattengränserna tangerar varandra bör omfattas och detta tillkännager riksdagen för regeringenmotiveringi propositionen föreslås bland annat enändring i mervärdesskattelagen i fråga omundantag från skatteplikt för omsättning av vissa fartyg samt förnödenheter till dessa innebörden av förslaget är att undantaget inskränks tillsådana fartyg som används på vad man definierar som öppna havet och som fraktar passagerare mot betalning eller används för handels industri eller fiskeriverksamhet samt fartyg som används för sjöräddningassistans eller kustfiskesjöfart som går mellan två stater där territorialvattengränserna tangerar varandra skulle därmed om man bokstavstolkar denna mening inte kunna omfattas av direktivets regel om skattebefrielse vid omsättning av fartyg som avses trafikera sådana farterur ett svenskt eller nordiskt perspektiv innebär denna snäva gränsdragning att rederier som hanterar våra mellanstatliga transportleder danmark sverige och sverige finland kommer att missgynnas i förhållande till rederier som trafikerar transportlinjer mellan till exempel polen eller tyskland och till exempel sverige eller finlandi kartan har vi gulmarkerat de områden där för sveriges del rederier skulle exkluderas från möjligheten att utnyttja fördelarna med momsfrihet här är det därför angeläget att komplettera förutsättningarna för momsbefrielsen vid omsättning av fartyg och i övrigt vad som avses i propositionen så att även fartyg som trafikerar eller avses trafikera områden mellan två stater där territorialvattengränserna tangerar varandra omfattas av lagändringenolle felten sd david lång sd'] 1921\n",
      "\n",
      " ['motion till riksdag olle felten och david lång båda sd med anledning av prop mervärde vid omsättning som avser fartyg samt deklaration i vissa fallförslag till riksdagsbeslut ställe sig bakom det som anför i motionbevistalan om att även sjöfart som går mellan två stater där territorialvattengränserna tangerar varan bör omfattande och detta tillkänna riksdag för regeringenmotivering proposition föreslå bland annat enändring i mervärdesskattelagen i fråga omundantag från skatteplikt för omsättning av vissa fartyg samt förnödenheter till dessa innebörd av förslag är att undan inskränkning tillsådana fartyg som används på vad man definierad som öppna havet och som fraktar passager mot betalning eller används för handel industri eller fiskeriverksamhet samt fartyg som används för sjöräddningassistans eller kustfiskesjöfart som går mellan två stater där territorialvattengränserna tangerar varan skull därmed om man bokstavstolkar denna mening inte kunna omfattande av direkt regel om skattebefrielse vid omsättning av fartyg som avses trafikenstraffskalan sådan farterur ett svenskt eller nordisk perspektiv innebär denna snäva gränsdragning att rederier som hantera våra mellan transportleder danmark sverige och sverige finland kommer att missgynna i förhålla till rederier som trafikerade transportlinjer mellan till exempel polen eller tyskland och till exempel sverige eller finland kartan har vi gulmarkerat de område där för sverigeroger del rederier skull exkludera från möjlig att utnyttja fördel med momsfrihet här är det därför angelägen att komplettera förutsättning för momsbefrielsen vid omsättning av fartyg och i övrig vad som avses i proposition så att även fartyg som trafikerade eller avses trafikenstraffskalan område mellan två stater där territorialvattengränserna tangerar varan omfattande av lagändring felten sd david lång sd'] 1857\n"
     ]
    }
   ],
   "source": [
    "#ta ut alla dokument\n",
    "jsondata = file_to_json(\"motioner.txt\")\n",
    "texts, orig_dic = json_to_vec(jsondata['data'])\n",
    "\n",
    "dic = build_dict(orig_dic)\n",
    "#print(dic)\n",
    "ts = texts[2:3]\n",
    "print(ts,len(ts[0]))\n",
    "texts = preprocess(ts,dic)\n",
    "\n",
    "print(\"\\n\",texts,len(texts[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nästa steg - Bag of words\n",
    "skapa vektor av alla ord. Eg bara köra list(set(orig_dic.values())) \n",
    "Skapa np.arrays där varje dokument har en siffra i varje position i ovan vektor.\n",
    "Detta blir då ett training example eller ett x i X.\n",
    "\n",
    "# NMF\n",
    "\n",
    "för att sedan få matrisrepresentation som går att köra NonNegative Matrix Factorization på.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
